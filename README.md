# ipex-llm-serve

This repo provides container images for **llama.cpp server** and **ollama server** for Intel CPU, GPU and NPU (based on [intelanalytics/ipex-llm-inference-cpp-xpu](https://hub.docker.com/r/intelanalytics/ipex-llm-inference-cpp-xpu)) that are ready-to-run.

| Name | Container Image | README |
|------|-----------------|--------|
| llama.cpp server | [ghcr.io/lirc572/ipex-llm-serve-llama.cpp](https://github.com/lirc572/ipex-llm-serve/pkgs/container/ipex-llm-serve-llama.cpp) | [README](https://github.com/lirc572/ipex-llm-serve/tree/main/llama.cpp) |
| ollama server | [ghcr.io/lirc572/ipex-llm-serve-ollama](https://github.com/lirc572/ipex-llm-serve/pkgs/container/ipex-llm-serve-ollama) | [README](https://github.com/lirc572/ipex-llm-serve/tree/main/ollama) |
